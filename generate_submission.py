#!/usr/bin/env python3
"""
generate_submission.py — experiment_log.csvの最良設定からKaggle提出用ipynbを自動生成

Usage:
    python generate_submission.py
    python generate_submission.py --log experiment_log.csv --output submission_kaggle.ipynb
"""

import argparse
import json
import os

import nbformat
import pandas as pd

from config import EXPERIMENT_CONFIGS


def find_best_config(log_path: str) -> tuple[str, dict]:
    """ログから最良設定を読み取り、設定辞書を返す"""
    df = pd.read_csv(log_path)
    best_row = df.nlargest(1, "geo_mean").iloc[0]
    exp_name = best_row["experiment"].replace("_p2", "")
    print(f"[INFO] Best experiment: {exp_name}")
    print(f"  BLEU: {best_row['bleu']}, chrF++: {best_row['chrf']}, GeoMean: {best_row['geo_mean']}")
    config = EXPERIMENT_CONFIGS[exp_name]
    return exp_name, config


def generate_notebook(exp_name: str, config: dict, output_path: str):
    """Kaggle提出用のipynbを生成"""
    nb = nbformat.v4.new_notebook()
    nb.metadata["kernelspec"] = {
        "display_name": "Python 3",
        "language": "python",
        "name": "python3",
    }

    cells = []

    # --- Cell 0: Title ---
    cells.append(nbformat.v4.new_markdown_cell(
        f"# Akkadian-to-English Translation\n"
        f"## Best model: `{config['model_name']}` ({exp_name})\n"
        f"Auto-generated by `generate_submission.py`"
    ))

    # --- Cell 1: Install dependencies ---
    install_packages = ["transformers", "sentencepiece", "protobuf", "sacrebleu"]
    if config.get("use_lora"):
        install_packages.extend(["peft", "bitsandbytes"])
    cells.append(nbformat.v4.new_code_cell(
        f"!pip install -q {' '.join(install_packages)}"
    ))

    # --- Cell 2: Imports ---
    imports = '''
import os
import re
import math
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup
from tqdm import tqdm
'''.strip()
    if config.get("use_lora"):
        imports += "\nfrom peft import LoraConfig, TaskType, get_peft_model"
    cells.append(nbformat.v4.new_code_cell(imports))

    # --- Cell 3: Configuration ---
    config_code = f'''
# === Configuration (auto-generated from best experiment: {exp_name}) ===
MODEL_NAME = "{config['model_name']}"
PREPROCESS = "{config['preprocess']}"
USE_DICT = {config.get('use_dict', False)}
USE_LORA = {config.get('use_lora', False)}
LR = {config['lr']}
BATCH_SIZE = {config['batch_size']}
MAX_SOURCE_LENGTH = {config['max_source_length']}
MAX_TARGET_LENGTH = {config['max_target_length']}
NUM_EPOCHS = 20
WEIGHT_DECAY = 0.01
WARMUP_RATIO = 0.1
TASK_PREFIX = "{config.get('task_prefix', '')}"
'''.strip()
    if config.get("use_lora"):
        config_code += f'''
LORA_R = {config.get('lora_r', 16)}
LORA_ALPHA = {config.get('lora_alpha', 32)}
LORA_DROPOUT = {config.get('lora_dropout', 0.05)}
'''
    if "src_lang" in config:
        config_code += f'\nSRC_LANG = "{config["src_lang"]}"'
        config_code += f'\nTGT_LANG = "{config["tgt_lang"]}"'

    config_code += '''

DATA_DIR = "/kaggle/input/deep-past-initiative-machine-translation"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")
'''
    cells.append(nbformat.v4.new_code_cell(config_code))

    # --- Cell 4: Preprocessing functions ---
    preprocess_code = '''
def preprocess_basic(text):
    text = text.strip()
    text = re.sub(r"\\s+", " ", text)
    return text

def preprocess_moderate(text):
    text = preprocess_basic(text)
    text = re.sub(r"\\[\\.{3,}\\]", " ", text)
    text = re.sub(r"\\[x\\]", " ", text, flags=re.IGNORECASE)
    text = re.sub(r"-{2,}", "-", text)
    text = re.sub(r"\\s+", " ", text).strip()
    return text

def preprocess_aggressive(text):
    text = preprocess_moderate(text)
    text = re.sub(r"[\\[\\]]", "", text)
    text = re.sub(r"\\([^)]*\\)", "", text)
    text = re.sub(r"\\s+", " ", text).strip()
    return text

PREPROCESS_FN = {
    "basic": preprocess_basic,
    "moderate": preprocess_moderate,
    "aggressive": preprocess_aggressive,
}
preprocess_fn = PREPROCESS_FN[PREPROCESS]
'''
    cells.append(nbformat.v4.new_code_cell(preprocess_code))

    # --- Cell 5: Dictionary (if used) ---
    if config.get("use_dict"):
        dict_code = '''
def load_dictionary(data_dir):
    for fname, sep in [("eBL_Dictionary.csv", ","), ("eBL_Dictionary.tsv", "\\t")]:
        dict_path = os.path.join(data_dir, fname)
        if os.path.exists(dict_path):
            df = pd.read_csv(dict_path, sep=sep, header=0, on_bad_lines="skip")
            break
    else:
        return {}
    mapping = {}
    for col_src, col_tgt in [("word", "definition"), ("lemma", "meaning"), ("cf", "gw")]:
        if col_src in df.columns and col_tgt in df.columns:
            for _, row in df.iterrows():
                src = str(row[col_src]).strip()
                tgt = str(row[col_tgt]).strip()
                if src and tgt and src != "nan" and tgt != "nan":
                    mapping[src] = tgt
            break
    print(f"Dictionary: {len(mapping)} entries")
    return mapping

def apply_dictionary_prefix(text, dictionary, max_hints=5):
    if not dictionary:
        return text
    words = text.split()
    hints = []
    for w in words:
        if w in dictionary and len(hints) < max_hints:
            hints.append(f"{w}={dictionary[w]}")
    if hints:
        return "Dictionary: " + "; ".join(hints) + " | " + text
    return text

dictionary = load_dictionary(DATA_DIR)
'''
        cells.append(nbformat.v4.new_code_cell(dict_code))

    # --- Cell 6: Dataset class ---
    dataset_code = '''
class TranslationDataset(Dataset):
    def __init__(self, sources, targets, tokenizer, max_source_length, max_target_length):
        self.sources = sources
        self.targets = targets
        self.tokenizer = tokenizer
        self.max_source_length = max_source_length
        self.max_target_length = max_target_length

    def __len__(self):
        return len(self.sources)

    def __getitem__(self, idx):
        source = self.sources[idx]
        target = self.targets[idx]
        source_enc = self.tokenizer(
            source, max_length=self.max_source_length,
            padding="max_length", truncation=True, return_tensors="pt",
        )
        target_enc = self.tokenizer(
            text_target=target, max_length=self.max_target_length,
            padding="max_length", truncation=True, return_tensors="pt",
        )
        labels = target_enc["input_ids"].squeeze()
        labels[labels == self.tokenizer.pad_token_id] = -100
        return {
            "input_ids": source_enc["input_ids"].squeeze(),
            "attention_mask": source_enc["attention_mask"].squeeze(),
            "labels": labels,
        }
'''
    cells.append(nbformat.v4.new_code_cell(dataset_code))

    # --- Cell 7: Load data ---
    load_data_code = '''
# Load data
train_df = pd.read_csv(os.path.join(DATA_DIR, "train.csv"))
test_df = pd.read_csv(os.path.join(DATA_DIR, "test.csv"))
print(f"Train: {len(train_df)}, Test: {len(test_df)}")
print(f"Train columns: {list(train_df.columns)}")
print(f"Test columns: {list(test_df.columns)}")

# Detect columns: train.csv = oare_id, transliteration, translation
#                  test.csv  = id, text_id, line_start, line_end, transliteration
src_col, tgt_col = None, None
for c in train_df.columns:
    cl = c.lower()
    if cl in ("input", "source", "akkadian", "src", "transliteration"):
        src_col = c
    elif cl in ("output", "target", "translation", "english", "tgt"):
        tgt_col = c
if src_col is None or tgt_col is None:
    cols = list(train_df.columns)
    src_col = cols[1] if len(cols) > 1 else cols[0]
    tgt_col = cols[2] if len(cols) > 2 else cols[-1]
print(f"Using: src={src_col}, tgt={tgt_col}")

sources = train_df[src_col].astype(str).tolist()
targets = train_df[tgt_col].astype(str).tolist()

# Test
test_src_col = None
for c in test_df.columns:
    if c.lower() in ("input", "source", "akkadian", "src", "transliteration"):
        test_src_col = c
if test_src_col is None:
    test_src_col = list(test_df.columns)[1] if len(test_df.columns) > 1 else list(test_df.columns)[0]
test_sources = test_df[test_src_col].astype(str).tolist()
test_ids = test_df.iloc[:, 0].tolist()
'''
    cells.append(nbformat.v4.new_code_cell(load_data_code))

    # --- Cell 8: Preprocess data ---
    preprocess_data_code = '''
# Apply preprocessing
train_src = [preprocess_fn(s) for s in sources]
train_tgt = [preprocess_fn(t) for t in targets]
test_src = [preprocess_fn(s) for s in test_sources]
'''
    if config.get("use_dict"):
        preprocess_data_code += '''
# Apply dictionary prefix
train_src = [apply_dictionary_prefix(s, dictionary) for s in train_src]
test_src = [apply_dictionary_prefix(s, dictionary) for s in test_src]
'''
    preprocess_data_code += '''
# Task prefix
if TASK_PREFIX:
    train_src = [TASK_PREFIX + s for s in train_src]
    test_src = [TASK_PREFIX + s for s in test_src]

print(f"Sample input: {train_src[0][:100]}")
print(f"Sample target: {train_tgt[0][:100]}")
'''
    cells.append(nbformat.v4.new_code_cell(preprocess_data_code))

    # --- Cell 9: Load model ---
    model_load_code = '''
# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
'''
    if "src_lang" in config:
        model_load_code += '''
try:
    tokenizer.src_lang = SRC_LANG
except Exception:
    print("Could not set src_lang, using default")
'''
    model_load_code += '''
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
'''
    if config.get("use_lora"):
        model_load_code += '''
lora_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,
    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
'''
    model_load_code += '''
model = model.to(device)
print(f"Model loaded: {MODEL_NAME}")
'''
    cells.append(nbformat.v4.new_code_cell(model_load_code))

    # --- Cell 10: Create datasets and dataloaders ---
    dataloader_code = '''
train_dataset = TranslationDataset(train_src, train_tgt, tokenizer, MAX_SOURCE_LENGTH, MAX_TARGET_LENGTH)
test_dataset = TranslationDataset(test_src, [""] * len(test_src), tokenizer, MAX_SOURCE_LENGTH, MAX_TARGET_LENGTH)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
print(f"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}")
'''
    cells.append(nbformat.v4.new_code_cell(dataloader_code))

    # --- Cell 11: Training ---
    training_code = '''
optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
total_steps = len(train_loader) * NUM_EPOCHS
warmup_steps = int(total_steps * WARMUP_RATIO)
scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)

scaler = torch.amp.GradScaler("cuda") if device.type == "cuda" else None

for epoch in range(NUM_EPOCHS):
    model.train()
    total_loss = 0
    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS}"):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        optimizer.zero_grad()
        if scaler is not None:
            with torch.amp.autocast("cuda"):
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()

        scheduler.step()
        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {avg_loss:.4f}")
'''
    cells.append(nbformat.v4.new_code_cell(training_code))

    # --- Cell 12: Inference ---
    inference_code = '''
model.eval()
all_preds = []

with torch.no_grad():
    for batch in tqdm(test_loader, desc="Predicting"):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        generated = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=MAX_TARGET_LENGTH,
            num_beams=4,
            early_stopping=True,
        )
        preds = tokenizer.batch_decode(generated, skip_special_tokens=True)
        all_preds.extend(preds)

print(f"Generated {len(all_preds)} predictions")
for i in range(min(5, len(all_preds))):
    print(f"  [{test_ids[i]}] {all_preds[i][:100]}")
'''
    cells.append(nbformat.v4.new_code_cell(inference_code))

    # --- Cell 13: Create submission ---
    submission_code = '''
submission = pd.DataFrame({"id": test_ids, "translation": all_preds})
submission.to_csv("submission.csv", index=False)
print(f"Submission saved: {len(submission)} rows")
submission.head(10)
'''
    cells.append(nbformat.v4.new_code_cell(submission_code))

    nb.cells = cells

    with open(output_path, "w", encoding="utf-8") as f:
        nbformat.write(nb, f)
    print(f"[INFO] Notebook saved to {output_path}")


def main():
    parser = argparse.ArgumentParser(description="Generate Kaggle submission notebook")
    parser.add_argument("--log", type=str, default="experiment_log.csv",
                        help="Path to experiment log CSV")
    parser.add_argument("--output", type=str, default="submission_kaggle.ipynb",
                        help="Output notebook path")
    args = parser.parse_args()

    if not os.path.exists(args.log):
        print(f"[ERROR] Experiment log not found: {args.log}")
        print("Run `python run_experiments.py` first to generate experiment results.")
        return

    exp_name, config = find_best_config(args.log)
    generate_notebook(exp_name, config, args.output)


if __name__ == "__main__":
    main()
